{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements cell\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sagemaker as sage\n",
    "import numpy as np\n",
    "import boto3\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "import os\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import mxnet as mx\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape raw CSV from NYTimes COVID-19 GitHub repo, parse with BeautifulSoup\n",
    "response = requests.get('https://github.com/nytimes/covid-19-data/blob/master/us-counties.csv')\n",
    "soup = BeautifulSoup(response.text)\n",
    "\n",
    "# put data from scraped page into pandas dataframe\n",
    "lines = [tr.find_all('td')[1] for tr in soup.find_all('tr')]\n",
    "raw_dict = {line['id'] : line.string.split(',') for line in lines}\n",
    "raw_labels = raw_dict['LC1']\n",
    "del raw_dict['LC1']\n",
    "raw_data = pd.DataFrame.from_dict(raw_dict, orient='index', columns=raw_labels)\n",
    "\n",
    "# remove datapoints without FIPS code\n",
    "trimmed_fips = raw_data[raw_data['fips'] != ''].copy(deep=True)\n",
    "trimmed_fips['fips'] = trimmed_fips['fips'].astype(int)\n",
    "trimmed_fips['cases'] = trimmed_fips['cases'].astype(int)\n",
    "trimmed_fips['deaths'] = trimmed_fips['deaths'].astype(int)\n",
    "trimmed_fips['date'] = trimmed_fips['date'].map(lambda date: datetime.strptime(date, '%Y-%m-%d').timestamp())\n",
    "trimmed_fips = trimmed_fips.drop(columns=['county', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload trimmed data from USDA\n",
    "education = pd.read_excel('Education.xls')\n",
    "# filter out states and countries\n",
    "education = education[education['FIPS Code'] % 1000 != 0]\n",
    "\n",
    "unemployment = pd.read_excel('Unemployment.xls')\n",
    "# filter out states and countries\n",
    "unemployment = unemployment[unemployment['FIPS'] % 1000 != 0]\n",
    "\n",
    "poverty = pd.read_excel('PovertyEstimates.xls')\n",
    "# filter out states and countries\n",
    "poverty = poverty[poverty['FIPS'] % 1000 != 0]\n",
    "\n",
    "population = pd.read_excel('PopulationEstimates.xls')\n",
    "# filter out states and countries\n",
    "population = population[population['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_people = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='People')\n",
    "# filter out states and countries\n",
    "atlas_people = atlas_people[atlas_people['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_jobs = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='Jobs')\n",
    "# filter out states and countries\n",
    "atlas_jobs = atlas_jobs[atlas_jobs['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_county_classifications = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='County Classifications')\n",
    "# filter out states and countries\n",
    "atlas_county = atlas_county_classifications[atlas_county_classifications['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_income = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='Income')\n",
    "# filter out states and countries\n",
    "atlas_income = atlas_income[atlas_income['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_veterans = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='Veterans')\n",
    "# filter out states and countries\n",
    "atlas_veterans = atlas_veterans[atlas_veterans['FIPS'] % 1000 != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join socioeconomic measures by county together with COVID-19 spread data\n",
    "\n",
    "data_with_edu = pd.merge(trimmed_fips, education, how='left', left_on='fips', right_on='FIPS Code')\n",
    "data_with_edu_unemploy = pd.merge(data_with_edu, unemployment, how='left', left_on='fips', right_on='FIPS')\n",
    "data_with_edu_unemploy_pov = pd.merge(\n",
    "    data_with_edu_unemploy, poverty, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_pop = pd.merge(\n",
    "    data_with_edu_unemploy_pov, population, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_people = pd.merge(\n",
    "    data_after_pop, atlas_people, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_jobs = pd.merge(\n",
    "    data_after_atlas_people, atlas_jobs, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_classif = pd.merge(\n",
    "    data_after_atlas_jobs, atlas_county_classifications, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_income = pd.merge(\n",
    "    data_after_atlas_classif, atlas_income, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_veterans = pd.merge(\n",
    "    data_after_atlas_income, atlas_veterans, how='left', left_on='fips', right_on='FIPS')\n",
    "data_with_usda = data_after_atlas_veterans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_usda = data_with_usda.drop(columns=['FIPS Code', 'FIPS_x', 'FIPS_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have NaNs, SageMaker doesn't like these\n",
    "# impute by mean of column \n",
    "# from https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\n",
    "\n",
    "def impute_by_means(a):\n",
    "    col_means = np.nanmean(a, axis=0)\n",
    "    inds = np.where(np.isnan(a))\n",
    "    a[inds] = np.take(col_means, inds[1])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features don't have cases or deaths\n",
    "\n",
    "train, test = train_test_split(data_with_usda)\n",
    "\n",
    "train_labels_cases = train['cases'].to_numpy(dtype=np.float32)\n",
    "train_labels_deaths = train['deaths'].to_numpy(dtype=np.float32)\n",
    "train_features = impute_by_means(train.drop(columns=['cases', 'deaths']).to_numpy(dtype=np.float32))\n",
    "\n",
    "test_labels_cases = test['cases'].to_numpy(dtype=np.float32)\n",
    "test_labels_deaths = test['deaths'].to_numpy(dtype=np.float32)\n",
    "test_features = impute_by_means(test.drop(columns=['cases', 'deaths']).to_numpy(dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = 'arn:aws:iam::022575370123:role/service-role/AmazonSageMaker-ExecutionRole-20200402T213912'\n",
    "bucket = 'sagemaker-studio-uok86wzhfvl'\n",
    "prefix = 'covid-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.bmc.com/blogs/aws-linear-learner/\n",
    "# and https://www.xaxis.com/insights/blog/steps-to-train-a-machine-learning-model-with-amazon-sagemaker-first-look/\n",
    "\n",
    "sess = sage.Session(default_bucket=bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'linearlearner'\n",
    "s3_train_data_cases = 's3://{}/{}/train_cases/{}'.format(bucket, prefix, key)\n",
    "s3_test_data_cases = 's3://{}/{}/test_cases/{}'.format(bucket, prefix, key)\n",
    "\n",
    "s3_train_data_deaths = 's3://{}/{}/train_deaths/{}'.format(bucket, prefix, key)\n",
    "s3_test_data_deaths = 's3://{}/{}/test_deaths/{}'.format(bucket, prefix, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't execute again unless data changed\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, train_features, train_labels_cases)\n",
    "buf.seek(0)\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train_cases', key)).upload_fileobj(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't execute again unless data changed\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, train_features, train_labels_deaths)\n",
    "buf.seek(0)\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train_deaths', key)).upload_fileobj(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't execute again unless data changed\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, test_features, test_labels_cases)\n",
    "buf.seek(0)\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test_cases', key)).upload_fileobj(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't execute again unless data changed\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, test_features, test_labels_deaths)\n",
    "buf.seek(0)\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test_deaths', key)).upload_fileobj(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_loc_cases = 's3://{}/{}/output_cases'.format(bucket, prefix)\n",
    "output_loc_deaths = 's3://{}/{}/output_deaths'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session(region_name='us-east-2').region_name, 'linear-learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_cases = sage.estimator.Estimator(container, role, train_instance_count=1,\n",
    "                                         train_instance_type='ml.c4.xlarge', output_path=output_loc_cases, \n",
    "                                         sagemaker_session=sess)\n",
    "estimator_cases.set_hyperparameters(feature_dim=train_features.shape[1], predictor_type='regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_deaths = sage.estimator.Estimator(container, role, train_instance_count=1,\n",
    "                                         train_instance_type='ml.c4.xlarge', output_path=output_loc_deaths, \n",
    "                                         sagemaker_session=sess)\n",
    "estimator_deaths.set_hyperparameters(feature_dim=train_features.shape[1], predictor_type='regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training job for cases\n",
    "\n",
    "estimator_cases.fit({'train': s3_train_data_cases, 'test': s3_test_data_cases}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training job for deaths\n",
    "\n",
    "estimator_deaths.fit({'train': s3_train_data_deaths, 'test': s3_test_data_deaths}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name_cases = estimator_cases.latest_training_job.job_name\n",
    "training_job_name_deaths = estimator_deaths.latest_training_job.job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_job_status(training_job_name: str):\n",
    "    job_info = boto3.client('sagemaker').describe_training_job(TrainingJobName=training_job_name)\n",
    "    job_status = job_info['TrainingJobStatus']\n",
    "    if job_status == 'Failed':\n",
    "        message = job_info['FailureReason']\n",
    "        print(f'Training failed with the following error: {message}')\n",
    "    return job_status, job_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Completed',\n",
       " {'TrainingJobName': 'linear-learner-2020-04-03-18-48-17-345',\n",
       "  'TrainingJobArn': 'arn:aws:sagemaker:us-east-2:022575370123:training-job/linear-learner-2020-04-03-18-48-17-345',\n",
       "  'ModelArtifacts': {'S3ModelArtifacts': 's3://sagemaker-studio-uok86wzhfvl/covid-data/output_deaths/linear-learner-2020-04-03-18-48-17-345/output/model.tar.gz'},\n",
       "  'TrainingJobStatus': 'Completed',\n",
       "  'SecondaryStatus': 'Completed',\n",
       "  'HyperParameters': {'feature_dim': '502', 'predictor_type': 'regressor'},\n",
       "  'AlgorithmSpecification': {'TrainingImage': '404615174143.dkr.ecr.us-east-2.amazonaws.com/linear-learner:1',\n",
       "   'TrainingInputMode': 'File',\n",
       "   'MetricDefinitions': [{'Name': 'test:dcg',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test dcg <score>=(\\\\S+)'},\n",
       "    {'Name': 'train:progress',\n",
       "     'Regex': '#progress_metric: host=\\\\S+, completed (\\\\S+) %'},\n",
       "    {'Name': 'test:binary_f_beta',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test binary_f_\\\\S+ <score>=(\\\\S+)'},\n",
       "    {'Name': 'train:objective_loss',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, train \\\\S+_objective <loss>=(\\\\S+)'},\n",
       "    {'Name': 'validation:macro_precision',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, validation macro_precision <score>=(\\\\S+)'},\n",
       "    {'Name': 'validation:dcg',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, validation dcg <score>=(\\\\S+)'},\n",
       "    {'Name': 'test:mse',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test mse <loss>=(\\\\S+)'},\n",
       "    {'Name': 'validation:binary_f_beta',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, validation binary_f_\\\\S+ <score>=(\\\\S+)'},\n",
       "    {'Name': 'validation:objective_loss',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, validation \\\\S+_objective <loss>=(\\\\S+)'},\n",
       "    {'Name': 'validation:objective_loss:final',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, validation \\\\S+_objective <loss>=(\\\\S+)'},\n",
       "    {'Name': 'test:macro_recall',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test macro_recall <score>=(\\\\S+)'},\n",
       "    {'Name': 'test:absolute_loss',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test absolute_loss <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:recall',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, train recall <score>=(\\\\S+)'},\n",
       "    {'Name': 'train:mse',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, train mse <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:precision',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, train precision <score>=(\\\\S+)'},\n",
       "    {'Name': 'train:objective_loss:final',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, train \\\\S+_objective <loss>=(\\\\S+)'},\n",
       "    {'Name': 'validation:recall',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, validation recall <score>=(\\\\S+)'},\n",
       "    {'Name': 'test:multiclass_accuracy',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test multiclass_accuracy <score>=(\\\\S+)'},\n",
       "    {'Name': 'validation:precision',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, validation precision <score>=(\\\\S+)'},\n",
       "    {'Name': 'validation:multiclass_accuracy',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, validation multiclass_accuracy <score>=(\\\\S+)'},\n",
       "    {'Name': 'train:binary_f_beta',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, train binary_f_\\\\S+ <score>=(\\\\S+)'},\n",
       "    {'Name': 'test:recall',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test recall <score>=(\\\\S+)'},\n",
       "    {'Name': 'test:macro_precision',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test macro_precision <score>=(\\\\S+)'},\n",
       "    {'Name': 'test:macro_f_beta',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test macro_f_\\\\S+ <score>=(\\\\S+)'},\n",
       "    {'Name': 'test:objective_loss',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test \\\\S+_objective <loss>=(\\\\S+)'},\n",
       "    {'Name': 'test:precision',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test precision <score>=(\\\\S+)'},\n",
       "    {'Name': 'validation:multiclass_top_k_accuracy',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, validation multiclass_top_k_accuracy_\\\\S+ <score>=(\\\\S+)'},\n",
       "    {'Name': 'train:binary_classification_accuracy',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, train binary_classification_accuracy <score>=(\\\\S+)'},\n",
       "    {'Name': 'validation:mse',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, validation mse <loss>=(\\\\S+)'},\n",
       "    {'Name': 'test:multiclass_top_k_accuracy',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test multiclass_top_k_accuracy_\\\\S+ <score>=(\\\\S+)'},\n",
       "    {'Name': 'validation:binary_classification_accuracy',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, validation binary_classification_accuracy <score>=(\\\\S+)'},\n",
       "    {'Name': 'train:absolute_loss',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, train absolute_loss <loss>=(\\\\S+)'},\n",
       "    {'Name': 'validation:macro_recall',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, validation macro_recall <score>=(\\\\S+)'},\n",
       "    {'Name': 'train:throughput',\n",
       "     'Regex': '#throughput_metric: host=\\\\S+, train throughput=(\\\\S+) records/second'},\n",
       "    {'Name': 'test:binary_classification_accuracy',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test binary_classification_accuracy <score>=(\\\\S+)'},\n",
       "    {'Name': 'validation:absolute_loss',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, validation absolute_loss <loss>=(\\\\S+)'},\n",
       "    {'Name': 'validation:macro_f_beta',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, validation macro_f_\\\\S+ <score>=(\\\\S+)'}],\n",
       "   'EnableSageMakerMetricsTimeSeries': False},\n",
       "  'RoleArn': 'arn:aws:iam::022575370123:role/service-role/AmazonSageMaker-ExecutionRole-20200402T213912',\n",
       "  'InputDataConfig': [{'ChannelName': 'train',\n",
       "    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "      'S3Uri': 's3://sagemaker-studio-uok86wzhfvl/covid-data/train_deaths/linearlearner',\n",
       "      'S3DataDistributionType': 'FullyReplicated'}},\n",
       "    'CompressionType': 'None',\n",
       "    'RecordWrapperType': 'None'},\n",
       "   {'ChannelName': 'test',\n",
       "    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "      'S3Uri': 's3://sagemaker-studio-uok86wzhfvl/covid-data/test_deaths/linearlearner',\n",
       "      'S3DataDistributionType': 'FullyReplicated'}},\n",
       "    'CompressionType': 'None',\n",
       "    'RecordWrapperType': 'None'}],\n",
       "  'OutputDataConfig': {'KmsKeyId': '',\n",
       "   'S3OutputPath': 's3://sagemaker-studio-uok86wzhfvl/covid-data/output_deaths'},\n",
       "  'ResourceConfig': {'InstanceType': 'ml.c4.xlarge',\n",
       "   'InstanceCount': 1,\n",
       "   'VolumeSizeInGB': 30},\n",
       "  'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "  'CreationTime': datetime.datetime(2020, 4, 3, 18, 48, 17, 507000, tzinfo=tzlocal()),\n",
       "  'TrainingStartTime': datetime.datetime(2020, 4, 3, 18, 50, 2, 785000, tzinfo=tzlocal()),\n",
       "  'TrainingEndTime': datetime.datetime(2020, 4, 3, 18, 51, 14, 508000, tzinfo=tzlocal()),\n",
       "  'LastModifiedTime': datetime.datetime(2020, 4, 3, 18, 51, 14, 508000, tzinfo=tzlocal()),\n",
       "  'SecondaryStatusTransitions': [{'Status': 'Starting',\n",
       "    'StartTime': datetime.datetime(2020, 4, 3, 18, 48, 17, 507000, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 3, 18, 50, 2, 785000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Preparing the instances for training'},\n",
       "   {'Status': 'Downloading',\n",
       "    'StartTime': datetime.datetime(2020, 4, 3, 18, 50, 2, 785000, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 3, 18, 50, 44, 297000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Downloading input data'},\n",
       "   {'Status': 'Training',\n",
       "    'StartTime': datetime.datetime(2020, 4, 3, 18, 50, 44, 297000, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 3, 18, 51, 7, 524000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Training image download completed. Training in progress.'},\n",
       "   {'Status': 'Uploading',\n",
       "    'StartTime': datetime.datetime(2020, 4, 3, 18, 51, 7, 524000, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 3, 18, 51, 14, 508000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Uploading generated training model'},\n",
       "   {'Status': 'Completed',\n",
       "    'StartTime': datetime.datetime(2020, 4, 3, 18, 51, 14, 508000, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 3, 18, 51, 14, 508000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Training job completed'}],\n",
       "  'FinalMetricDataList': [{'MetricName': 'train:progress',\n",
       "    'Value': 100.0,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 32, 19, 866000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:objective_loss',\n",
       "    'Value': 0.8648949265480042,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 32, 19, 866000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'test:mse',\n",
       "    'Value': 14.211689949035645,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 32, 19, 866000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'test:absolute_loss',\n",
       "    'Value': 1.4213160276412964,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 32, 19, 866000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:mse',\n",
       "    'Value': 14.269291877746582,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 32, 19, 866000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:objective_loss:final',\n",
       "    'Value': 14.269291877746582,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 32, 19, 866000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'test:objective_loss',\n",
       "    'Value': 14.211689949035645,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 32, 19, 866000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:absolute_loss',\n",
       "    'Value': 1.4078681468963623,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 32, 19, 866000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:throughput',\n",
       "    'Value': 50281.625,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 32, 19, 866000, tzinfo=tzlocal())}],\n",
       "  'EnableNetworkIsolation': False,\n",
       "  'EnableInterContainerTrafficEncryption': False,\n",
       "  'EnableManagedSpotTraining': False,\n",
       "  'TrainingTimeInSeconds': 72,\n",
       "  'BillableTimeInSeconds': 72,\n",
       "  'ResponseMetadata': {'RequestId': '489607eb-2de3-471c-b429-14dad4e075f2',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amzn-requestid': '489607eb-2de3-471c-b429-14dad4e075f2',\n",
       "    'content-type': 'application/x-amz-json-1.1',\n",
       "    'content-length': '7510',\n",
       "    'date': 'Fri, 03 Apr 2020 18:53:20 GMT'},\n",
       "   'RetryAttempts': 0}})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_training_job_status(estimator_deaths.latest_training_job.job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: linear-learner-2020-04-03-18-23-02-289\n"
     ]
    }
   ],
   "source": [
    "linear_predictor_cases = estimator_cases.deploy(initial_instance_count=1, wait=False, \n",
    "                                   instance_type='ml.c4.xlarge', endpoint_name='predict-covid19-cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predictor_deaths = estimator_deaths.deploy(initial_instance_count=1, wait=False,\n",
    "                                   instance_type='ml.c4.xlarge', endpoint_name='predict-covid19-deaths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predictor_cases.content_type = 'text/csv'\n",
    "linear_predictor_cases.serializer = csv_serializer\n",
    "linear_predictor_cases.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predictor_deaths.content_type = 'text/csv'\n",
    "linear_predictor_deaths.serializer = csv_serializer\n",
    "linear_predictor_deaths.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join socioeconomic measures by county together\n",
    "\n",
    "data_with_edu_unemploy = pd.merge(education, unemployment, how='left', left_on='FIPS Code', right_on='FIPS')\n",
    "data_with_edu_unemploy_pov = pd.merge(\n",
    "    data_with_edu_unemploy, poverty, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_pop = pd.merge(\n",
    "    data_with_edu_unemploy_pov, population, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_people = pd.merge(\n",
    "    data_after_pop, atlas_people, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_jobs = pd.merge(\n",
    "    data_after_atlas_people, atlas_jobs, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_classif = pd.merge(\n",
    "    data_after_atlas_jobs, atlas_county_classifications, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_income = pd.merge(\n",
    "    data_after_atlas_classif, atlas_income, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_veterans = pd.merge(\n",
    "    data_after_atlas_income, atlas_veterans, how='left', left_on='FIPS', right_on='FIPS')\n",
    "county_data = data_after_atlas_veterans.drop(columns=['FIPS Code'])\n",
    "county_data = county_data.set_index('FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a feature should have: date (Unix format), fips number, county data\n",
    "def get_prediction(date, fips, label='cases'):\n",
    "    a = np.array([date, fips], dtype=np.float32)\n",
    "    b = county_data.loc[fips].to_numpy(np.float32)\n",
    "    feature = np.concatenate((a, b))\n",
    "    if label == 'cases':\n",
    "        return linear_predictor_cases.predict(feature)\n",
    "    elif label == 'deaths':\n",
    "        return linear_predictor_deaths.predict(feature)\n",
    "    else: \n",
    "        raise ValueError('label must be either cases or deaths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c26501500a7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m39153\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cases'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "get_prediction(datetime.now().timestamp(), 39153, label='cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:environment/datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
