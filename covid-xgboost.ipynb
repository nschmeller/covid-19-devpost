{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements cell\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sagemaker as sage\n",
    "import numpy as np\n",
    "import boto3\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "import os\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import mxnet as mx\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from io import StringIO\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape raw CSV from NYTimes COVID-19 GitHub repo, parse with BeautifulSoup\n",
    "response = requests.get('https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv')\n",
    "\n",
    "text = StringIO(response.text)\n",
    "raw_data = pd.read_csv(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove datapoints without FIPS code\n",
    "# this includes New York City, that's a problem\n",
    "\n",
    "trimmed_fips = raw_data[np.isfinite(raw_data['fips'])].copy(deep=True)\n",
    "trimmed_fips['fips'] = trimmed_fips['fips'].astype(int)\n",
    "trimmed_fips['cases'] = trimmed_fips['cases'].astype(int)\n",
    "trimmed_fips['deaths'] = trimmed_fips['deaths'].astype(int)\n",
    "trimmed_fips['date'] = trimmed_fips['date'].map(lambda date: datetime.strptime(date, '%Y-%m-%d').timestamp())\n",
    "trimmed_fips = trimmed_fips.drop(columns=['county', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload trimmed data from USDA\n",
    "education = pd.read_excel('Education.xls')\n",
    "# filter out states and countries\n",
    "education = education[education['FIPS Code'] % 1000 != 0]\n",
    "\n",
    "unemployment = pd.read_excel('Unemployment.xls')\n",
    "# filter out states and countries\n",
    "unemployment = unemployment[unemployment['FIPS'] % 1000 != 0]\n",
    "\n",
    "poverty = pd.read_excel('PovertyEstimates.xls')\n",
    "# filter out states and countries\n",
    "poverty = poverty[poverty['FIPS'] % 1000 != 0]\n",
    "\n",
    "population = pd.read_excel('PopulationEstimates.xls')\n",
    "# filter out states and countries\n",
    "population = population[population['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_people = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='People')\n",
    "# filter out states and countries\n",
    "atlas_people = atlas_people[atlas_people['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_jobs = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='Jobs')\n",
    "# filter out states and countries\n",
    "atlas_jobs = atlas_jobs[atlas_jobs['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_county_classifications = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='County Classifications')\n",
    "# filter out states and countries\n",
    "atlas_county = atlas_county_classifications[atlas_county_classifications['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_income = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='Income')\n",
    "# filter out states and countries\n",
    "atlas_income = atlas_income[atlas_income['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_veterans = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='Veterans')\n",
    "# filter out states and countries\n",
    "atlas_veterans = atlas_veterans[atlas_veterans['FIPS'] % 1000 != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join socioeconomic measures by county together with COVID-19 spread data\n",
    "\n",
    "data_with_edu = pd.merge(trimmed_fips, education, how='left', left_on='fips', right_on='FIPS Code')\n",
    "data_with_edu_unemploy = pd.merge(data_with_edu, unemployment, how='left', left_on='fips', right_on='FIPS')\n",
    "data_with_edu_unemploy_pov = pd.merge(\n",
    "    data_with_edu_unemploy, poverty, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_pop = pd.merge(\n",
    "    data_with_edu_unemploy_pov, population, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_people = pd.merge(\n",
    "    data_after_pop, atlas_people, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_jobs = pd.merge(\n",
    "    data_after_atlas_people, atlas_jobs, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_classif = pd.merge(\n",
    "    data_after_atlas_jobs, atlas_county_classifications, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_income = pd.merge(\n",
    "    data_after_atlas_classif, atlas_income, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_veterans = pd.merge(\n",
    "    data_after_atlas_income, atlas_veterans, how='left', left_on='fips', right_on='FIPS')\n",
    "data_with_usda = data_after_atlas_veterans\n",
    "data_with_usda = data_with_usda.drop(columns=['FIPS Code', 'FIPS_x', 'FIPS_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have some NaNs, SageMaker doesn't like these\n",
    "# impute by mean of each column \n",
    "# from https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\n",
    "\n",
    "def impute_by_means(a):\n",
    "    col_means = np.nanmean(a, axis=0)\n",
    "    inds = np.where(np.isnan(a))\n",
    "    a[inds] = np.take(col_means, inds[1])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features don't have cases or deaths\n",
    "# need to convert to CSV for XGBoost\n",
    "\n",
    "train, test = train_test_split(data_with_usda)\n",
    "\n",
    "train_labels_cases = train['cases']\n",
    "train_labels_deaths = train['deaths']\n",
    "train_features = train.drop(columns=['cases', 'deaths'])\n",
    "train_cases = train_features.copy(deep=True)\n",
    "train_cases.insert(0, 'cases', train_labels_cases)\n",
    "train_cases = train_cases.to_numpy(np.float32)\n",
    "train_deaths = train_features.copy(deep=True)\n",
    "train_deaths.insert(0, 'deaths', train_labels_deaths)\n",
    "train_deaths = train_deaths.to_numpy(np.float32)\n",
    "np.savetxt('train_cases.csv', impute_by_means(train_cases), delimiter=',')\n",
    "np.savetxt('train_deaths.csv', impute_by_means(train_deaths), delimiter=',')\n",
    "\n",
    "test_labels_cases = test['cases']\n",
    "test_labels_deaths = test['deaths']\n",
    "test_features = test.drop(columns=['cases', 'deaths'])\n",
    "test_cases = test_features.copy(deep=True)\n",
    "test_cases.insert(0, 'cases', test_labels_cases)\n",
    "test_cases = test_cases.to_numpy(np.float32)\n",
    "test_deaths = test_features.copy(deep=True)\n",
    "test_deaths.insert(0, 'deaths', test_labels_deaths)\n",
    "test_deaths = test_deaths.to_numpy(np.float32)\n",
    "np.savetxt('test_cases.csv', impute_by_means(test_cases), delimiter=',')\n",
    "np.savetxt('test_deaths.csv', impute_by_means(test_deaths), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = 'arn:aws:iam::022575370123:role/service-role/AmazonSageMaker-ExecutionRole-20200402T213912'\n",
    "bucket = 'sagemaker-studio-uok86wzhfvl'\n",
    "prefix = 'covid-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.bmc.com/blogs/aws-linear-learner/\n",
    "# and https://www.xaxis.com/insights/blog/steps-to-train-a-machine-learning-model-with-amazon-sagemaker-first-look/\n",
    "\n",
    "sess = sage.Session(default_bucket=bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'xgboost'\n",
    "s3_train_cases = 's3://{}/{}/train_cases/{}/train_cases.csv'.format(bucket, prefix, key)\n",
    "s3_test_cases = 's3://{}/{}/test_cases/{}/test_cases.csv'.format(bucket, prefix, key)\n",
    "\n",
    "s3_train_deaths = 's3://{}/{}/train_deaths/{}/train_deaths.csv'.format(bucket, prefix, key)\n",
    "s3_test_deaths = 's3://{}/{}/test_deaths/{}/test_deaths.csv'.format(bucket, prefix, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't execute again unless data changed\n",
    "boto3.resource('s3').Bucket(bucket).Object(\n",
    "    os.path.join(prefix, 'train_cases', key, 'train_cases.csv')).upload_file('train_cases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't execute again unless data changed\n",
    "boto3.resource('s3').Bucket(bucket).Object(\n",
    "    os.path.join(prefix, 'train_deaths', key, 'train_deaths.csv')).upload_file('train_deaths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't execute again unless data changed\n",
    "boto3.resource('s3').Bucket(bucket).Object(\n",
    "    os.path.join(prefix, 'test_cases', key, 'test_cases.csv')).upload_file('test_cases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't execute again unless data changed\n",
    "boto3.resource('s3').Bucket(bucket).Object(\n",
    "    os.path.join(prefix, 'test_deaths', key, 'test_deaths.csv')).upload_file('test_deaths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_loc_cases = 's3://{}/{}/output_cases'.format(bucket, prefix)\n",
    "output_loc_deaths = 's3://{}/{}/output_deaths'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session(region_name='us-east-2').region_name, 'xgboost', \n",
    "                         repo_version='0.90-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train_cases = sage.s3_input(s3_data=s3_train_cases, content_type='csv')\n",
    "s3_input_train_deaths = sage.s3_input(s3_data=s3_train_deaths, content_type='csv')\n",
    "s3_input_test_cases = sage.s3_input(s3_data=s3_test_cases, content_type='csv')\n",
    "s3_input_test_deaths = sage.s3_input(s3_data=s3_test_deaths, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_cases = sage.estimator.Estimator(container, role, train_instance_count=1,\n",
    "                                         train_instance_type='ml.c4.xlarge', output_path=output_loc_cases, \n",
    "                                         sagemaker_session=sess)\n",
    "estimator_cases.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        num_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_deaths = sage.estimator.Estimator(container, role, train_instance_count=1,\n",
    "                                         train_instance_type='ml.c4.xlarge', output_path=output_loc_deaths, \n",
    "                                         sagemaker_session=sess)\n",
    "estimator_deaths.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        num_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training job for cases\n",
    "# don't run if not needed\n",
    "\n",
    "estimator_cases.fit({'train': s3_input_train_cases, 'validation': s3_input_test_cases}, wait=False)\n",
    "training_job_name_cases = estimator_cases.latest_training_job.job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training job for deaths\n",
    "# don't run if not needed\n",
    "\n",
    "estimator_deaths.fit({'train': s3_input_train_deaths, 'validation': s3_input_test_deaths}, wait=False)\n",
    "training_job_name_deaths = estimator_deaths.latest_training_job.job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_job_status(training_job_name: str):\n",
    "    job_info = boto3.client('sagemaker').describe_training_job(TrainingJobName=training_job_name)\n",
    "    job_status = job_info['TrainingJobStatus']\n",
    "    if job_status == 'Failed':\n",
    "        message = job_info['FailureReason']\n",
    "        print(f'Training failed with the following error: {message}')\n",
    "    return job_status, job_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Completed',\n",
       " {'TrainingJobName': 'sagemaker-xgboost-2020-04-04-16-51-49-513',\n",
       "  'TrainingJobArn': 'arn:aws:sagemaker:us-east-2:022575370123:training-job/sagemaker-xgboost-2020-04-04-16-51-49-513',\n",
       "  'ModelArtifacts': {'S3ModelArtifacts': 's3://sagemaker-studio-uok86wzhfvl/covid-data/output_deaths/sagemaker-xgboost-2020-04-04-16-51-49-513/output/model.tar.gz'},\n",
       "  'TrainingJobStatus': 'Completed',\n",
       "  'SecondaryStatus': 'Completed',\n",
       "  'HyperParameters': {'eta': '0.2',\n",
       "   'gamma': '4',\n",
       "   'max_depth': '5',\n",
       "   'min_child_weight': '6',\n",
       "   'num_round': '100',\n",
       "   'subsample': '0.8'},\n",
       "  'AlgorithmSpecification': {'TrainingImage': '257758044811.dkr.ecr.us-east-2.amazonaws.com/sagemaker-xgboost:0.90-1-cpu-py3',\n",
       "   'TrainingInputMode': 'File',\n",
       "   'MetricDefinitions': [{'Name': 'train:mae',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-mae:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:merror',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-merror:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:gamma-nloglik',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-gamma-nloglik:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:mae',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-mae:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:logloss',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-logloss:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:mlogloss',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-mlogloss:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:f1',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-f1:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:accuracy',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-accuracy:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:mse',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-mse:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:poisson-nloglik',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-poisson-nloglik:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:tweedie-nloglik',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-tweedie-nloglik:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:error',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-error:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:ndcg',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-ndcg:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:map',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-map:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:auc',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-auc:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:gamma-deviance',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-gamma-deviance:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:auc',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-auc:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:error',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-error:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:merror',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-merror:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:poisson-nloglik',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-poisson-nloglik:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:rmse',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-rmse:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:logloss',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-logloss:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:accuracy',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-accuracy:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:tweedie-nloglik',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-tweedie-nloglik:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:rmse',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-rmse:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:gamma-deviance',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-gamma-deviance:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:mse',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-mse:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:ndcg',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-ndcg:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:f1',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-f1:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:mlogloss',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-mlogloss:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'train:map',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011train-map:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'},\n",
       "    {'Name': 'validation:gamma-nloglik',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-gamma-nloglik:([-+]?[0-9]*\\\\.?[0-9]+([eE][-+]?[0-9]+)?).*'}],\n",
       "   'EnableSageMakerMetricsTimeSeries': False},\n",
       "  'RoleArn': 'arn:aws:iam::022575370123:role/service-role/AmazonSageMaker-ExecutionRole-20200402T213912',\n",
       "  'InputDataConfig': [{'ChannelName': 'train',\n",
       "    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "      'S3Uri': 's3://sagemaker-studio-uok86wzhfvl/covid-data/train_deaths/xgboost/train_deaths.csv',\n",
       "      'S3DataDistributionType': 'FullyReplicated'}},\n",
       "    'ContentType': 'csv',\n",
       "    'CompressionType': 'None',\n",
       "    'RecordWrapperType': 'None'},\n",
       "   {'ChannelName': 'validation',\n",
       "    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "      'S3Uri': 's3://sagemaker-studio-uok86wzhfvl/covid-data/test_deaths/xgboost/test_deaths.csv',\n",
       "      'S3DataDistributionType': 'FullyReplicated'}},\n",
       "    'ContentType': 'csv',\n",
       "    'CompressionType': 'None',\n",
       "    'RecordWrapperType': 'None'}],\n",
       "  'OutputDataConfig': {'KmsKeyId': '',\n",
       "   'S3OutputPath': 's3://sagemaker-studio-uok86wzhfvl/covid-data/output_deaths'},\n",
       "  'ResourceConfig': {'InstanceType': 'ml.c4.xlarge',\n",
       "   'InstanceCount': 1,\n",
       "   'VolumeSizeInGB': 30},\n",
       "  'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "  'CreationTime': datetime.datetime(2020, 4, 4, 16, 51, 49, 642000, tzinfo=tzlocal()),\n",
       "  'TrainingStartTime': datetime.datetime(2020, 4, 4, 16, 53, 34, 432000, tzinfo=tzlocal()),\n",
       "  'TrainingEndTime': datetime.datetime(2020, 4, 4, 16, 54, 59, 729000, tzinfo=tzlocal()),\n",
       "  'LastModifiedTime': datetime.datetime(2020, 4, 4, 16, 54, 59, 729000, tzinfo=tzlocal()),\n",
       "  'SecondaryStatusTransitions': [{'Status': 'Starting',\n",
       "    'StartTime': datetime.datetime(2020, 4, 4, 16, 51, 49, 642000, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 4, 16, 53, 34, 432000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Preparing the instances for training'},\n",
       "   {'Status': 'Downloading',\n",
       "    'StartTime': datetime.datetime(2020, 4, 4, 16, 53, 34, 432000, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 4, 16, 54, 5, 427000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Downloading input data'},\n",
       "   {'Status': 'Training',\n",
       "    'StartTime': datetime.datetime(2020, 4, 4, 16, 54, 5, 427000, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 4, 16, 54, 52, 905000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Training image download completed. Training in progress.'},\n",
       "   {'Status': 'Uploading',\n",
       "    'StartTime': datetime.datetime(2020, 4, 4, 16, 54, 52, 905000, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 4, 16, 54, 59, 729000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Uploading generated training model'},\n",
       "   {'Status': 'Completed',\n",
       "    'StartTime': datetime.datetime(2020, 4, 4, 16, 54, 59, 729000, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 4, 16, 54, 59, 729000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Training job completed'}],\n",
       "  'FinalMetricDataList': [{'MetricName': 'train:rmse',\n",
       "    'Value': 0.5722240209579468,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 33, 39, 291000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'validation:rmse',\n",
       "    'Value': 1.439520001411438,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 33, 39, 291000, tzinfo=tzlocal())}],\n",
       "  'EnableNetworkIsolation': False,\n",
       "  'EnableInterContainerTrafficEncryption': False,\n",
       "  'EnableManagedSpotTraining': False,\n",
       "  'TrainingTimeInSeconds': 85,\n",
       "  'BillableTimeInSeconds': 85,\n",
       "  'ResponseMetadata': {'RequestId': 'a6c507cd-111d-4571-8cb6-09f1ec26af35',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amzn-requestid': 'a6c507cd-111d-4571-8cb6-09f1ec26af35',\n",
       "    'content-type': 'application/x-amz-json-1.1',\n",
       "    'content-length': '6862',\n",
       "    'date': 'Sat, 04 Apr 2020 16:56:48 GMT'},\n",
       "   'RetryAttempts': 0}})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_training_job_status(estimator_deaths.latest_training_job.job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_predictor_cases = estimator_cases.deploy(initial_instance_count=1, wait=False, \n",
    "                                   instance_type='ml.c4.xlarge', endpoint_name='xgboost-covid19-cases')\n",
    "xgboost_predictor_cases.content_type = 'text/csv'\n",
    "xgboost_predictor_cases.serializer = csv_serializer\n",
    "xgboost_predictor_cases.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_predictor_deaths = estimator_deaths.deploy(initial_instance_count=1, wait=False,\n",
    "                                   instance_type='ml.c4.xlarge', endpoint_name='xgboost-covid19-deaths')\n",
    "xgboost_predictor_deaths.content_type = 'text/csv'\n",
    "xgboost_predictor_deaths.serializer = csv_serializer\n",
    "xgboost_predictor_deaths.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join socioeconomic measures by county together\n",
    "\n",
    "data_with_edu_unemploy = pd.merge(education, unemployment, how='left', left_on='FIPS Code', right_on='FIPS')\n",
    "data_with_edu_unemploy_pov = pd.merge(\n",
    "    data_with_edu_unemploy, poverty, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_pop = pd.merge(\n",
    "    data_with_edu_unemploy_pov, population, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_people = pd.merge(\n",
    "    data_after_pop, atlas_people, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_jobs = pd.merge(\n",
    "    data_after_atlas_people, atlas_jobs, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_classif = pd.merge(\n",
    "    data_after_atlas_jobs, atlas_county_classifications, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_income = pd.merge(\n",
    "    data_after_atlas_classif, atlas_income, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_veterans = pd.merge(\n",
    "    data_after_atlas_income, atlas_veterans, how='left', left_on='FIPS', right_on='FIPS')\n",
    "county_data = data_after_atlas_veterans.drop(columns=['FIPS Code'])\n",
    "county_data = county_data.set_index('FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker-runtime')\n",
    "def linear_predictor(feature, label='cases'):\n",
    "    csv_feature = StringIO('')\n",
    "    df_temp = pd.DataFrame(feature)\n",
    "    df_temp.transpose().to_csv(csv_feature, header=False, index=False)\n",
    "    response = client.invoke_endpoint(EndpointName='predict-covid19-{}'.format(label),\n",
    "                          Body=csv_feature.getvalue(), ContentType='text/csv')\n",
    "    csv_feature.close()\n",
    "    return json.load(response['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_predictor(feature, label='cases'):\n",
    "    csv_feature = StringIO('')\n",
    "    df_temp = pd.DataFrame(feature)\n",
    "    df_temp.transpose().to_csv(csv_feature, header=False, index=False)\n",
    "    response = client.invoke_endpoint(EndpointName='xgboost-covid19-{}'.format(label),\n",
    "                                     Body=csv_feature.getvalue(), ContentType='text/csv')\n",
    "    csv_feature.close()\n",
    "    return json.load(response['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a feature should have: date (Unix format), fips number, county data\n",
    "def get_linear_prediction(date, fips, label='cases'):\n",
    "    a = np.array([date, fips], dtype=np.float32)\n",
    "    b = county_data.loc[fips].to_numpy(np.float32)\n",
    "    feature = np.concatenate((a, b))\n",
    "    return linear_predictor(feature, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a feature should have: date (Unix format), fips number, county data\n",
    "def get_xgboost_prediction(date, fips, label='cases'):\n",
    "    a = np.array([date, fips], dtype=np.float32)\n",
    "    b = county_data.loc[fips].to_numpy(np.float32)\n",
    "    feature = np.concatenate((a, b))\n",
    "    return xgboost_predictor(feature, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'score': 166.28125}]}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_linear_prediction((datetime.now() + timedelta(0)).timestamp(), 39103, label='cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.50457000732422"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_xgboost_prediction((datetime.now() + timedelta(-10)).timestamp(), 39103, label='cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:environment/datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
