{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements cell\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sagemaker as sage\n",
    "import numpy as np\n",
    "import boto3\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "import os\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import mxnet as mx\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from io import StringIO\n",
    "import json\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import json_serializer, json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape raw CSV from NYTimes COVID-19 GitHub repo, parse with BeautifulSoup\n",
    "response = requests.get('https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv')\n",
    "\n",
    "text = StringIO(response.text)\n",
    "raw_data = pd.read_csv(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove datapoints without FIPS code\n",
    "# this includes New York City, that's a problem\n",
    "\n",
    "trimmed_fips = raw_data[np.isfinite(raw_data['fips'])].copy(deep=True)\n",
    "trimmed_fips['fips'] = trimmed_fips['fips'].astype(int)\n",
    "trimmed_fips['cases'] = trimmed_fips['cases'].astype(int)\n",
    "trimmed_fips['deaths'] = trimmed_fips['deaths'].astype(int)\n",
    "trimmed_fips['date'] = trimmed_fips['date'].map(lambda date: datetime.strptime(date, '%Y-%m-%d').timestamp())\n",
    "trimmed_fips = trimmed_fips.drop(columns=['county', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload trimmed data from USDA\n",
    "education = pd.read_excel('Education.xls')\n",
    "# filter out states and countries\n",
    "education = education[education['FIPS Code'] % 1000 != 0]\n",
    "\n",
    "unemployment = pd.read_excel('Unemployment.xls')\n",
    "# filter out states and countries\n",
    "unemployment = unemployment[unemployment['FIPS'] % 1000 != 0]\n",
    "\n",
    "poverty = pd.read_excel('PovertyEstimates.xls')\n",
    "# filter out states and countries\n",
    "poverty = poverty[poverty['FIPS'] % 1000 != 0]\n",
    "\n",
    "population = pd.read_excel('PopulationEstimates.xls')\n",
    "# filter out states and countries\n",
    "population = population[population['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_people = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='People')\n",
    "# filter out states and countries\n",
    "atlas_people = atlas_people[atlas_people['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_jobs = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='Jobs')\n",
    "# filter out states and countries\n",
    "atlas_jobs = atlas_jobs[atlas_jobs['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_county_classifications = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='County Classifications')\n",
    "# filter out states and countries\n",
    "atlas_county = atlas_county_classifications[atlas_county_classifications['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_income = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='Income')\n",
    "# filter out states and countries\n",
    "atlas_income = atlas_income[atlas_income['FIPS'] % 1000 != 0]\n",
    "\n",
    "atlas_veterans = pd.read_excel('RuralAtlasData20.xlsx', sheet_name='Veterans')\n",
    "# filter out states and countries\n",
    "atlas_veterans = atlas_veterans[atlas_veterans['FIPS'] % 1000 != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join socioeconomic measures by county together with COVID-19 spread data\n",
    "\n",
    "data_with_edu = pd.merge(trimmed_fips, education, how='left', left_on='fips', right_on='FIPS Code')\n",
    "data_with_edu_unemploy = pd.merge(data_with_edu, unemployment, how='left', left_on='fips', right_on='FIPS')\n",
    "data_with_edu_unemploy_pov = pd.merge(\n",
    "    data_with_edu_unemploy, poverty, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_pop = pd.merge(\n",
    "    data_with_edu_unemploy_pov, population, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_people = pd.merge(\n",
    "    data_after_pop, atlas_people, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_jobs = pd.merge(\n",
    "    data_after_atlas_people, atlas_jobs, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_classif = pd.merge(\n",
    "    data_after_atlas_jobs, atlas_county_classifications, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_income = pd.merge(\n",
    "    data_after_atlas_classif, atlas_income, how='left', left_on='fips', right_on='FIPS')\n",
    "data_after_atlas_veterans = pd.merge(\n",
    "    data_after_atlas_income, atlas_veterans, how='left', left_on='fips', right_on='FIPS')\n",
    "data_with_usda = data_after_atlas_veterans\n",
    "data_with_usda = data_with_usda.drop(columns=['FIPS Code', 'FIPS_x', 'FIPS_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have some NaNs, SageMaker doesn't like these\n",
    "# impute by mean of each column \n",
    "# from https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\n",
    "\n",
    "def impute_by_means(a):\n",
    "    col_means = np.nanmean(a, axis=0)\n",
    "    inds = np.where(np.isnan(a))\n",
    "    a[inds] = np.take(col_means, inds[1])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features don't have cases or deaths\n",
    "\n",
    "train, test = train_test_split(data_with_usda)\n",
    "\n",
    "train_labels_cases = train['cases'].to_numpy(dtype=np.float32)\n",
    "train_labels_deaths = train['deaths'].to_numpy(dtype=np.float32)\n",
    "train_features = impute_by_means(train.drop(columns=['cases', 'deaths']).to_numpy(dtype=np.float32))\n",
    "\n",
    "test_labels_cases = test['cases'].to_numpy(dtype=np.float32)\n",
    "test_labels_deaths = test['deaths'].to_numpy(dtype=np.float32)\n",
    "test_features = impute_by_means(test.drop(columns=['cases', 'deaths']).to_numpy(dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "bucket = 'sagemaker-studio-uok86wzhfvl'\n",
    "prefix = 'covid-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.bmc.com/blogs/aws-linear-learner/\n",
    "# and https://www.xaxis.com/insights/blog/steps-to-train-a-machine-learning-model-with-amazon-sagemaker-first-look/\n",
    "\n",
    "sess = sage.Session(default_bucket=bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'fm'\n",
    "s3_train_data_cases = 's3://{}/{}/train_cases/{}'.format(bucket, prefix, key)\n",
    "s3_test_data_cases = 's3://{}/{}/test_cases/{}'.format(bucket, prefix, key)\n",
    "\n",
    "s3_train_data_deaths = 's3://{}/{}/train_deaths/{}'.format(bucket, prefix, key)\n",
    "s3_test_data_deaths = 's3://{}/{}/test_deaths/{}'.format(bucket, prefix, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't execute again unless data changed\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, train_features, train_labels_cases)\n",
    "buf.seek(0)\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train_cases', key)).upload_fileobj(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't execute again unless data changed\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, train_features, train_labels_deaths)\n",
    "buf.seek(0)\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train_deaths', key)).upload_fileobj(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't execute again unless data changed\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, test_features, test_labels_cases)\n",
    "buf.seek(0)\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test_cases', key)).upload_fileobj(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't execute again unless data changed\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, test_features, test_labels_deaths)\n",
    "buf.seek(0)\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test_deaths', key)).upload_fileobj(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_loc_cases = 's3://{}/{}/output_cases'.format(bucket, prefix)\n",
    "output_loc_deaths = 's3://{}/{}/output_deaths'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'factorization-machines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_cases = sage.estimator.Estimator(container, role, train_instance_count=1,\n",
    "                                         train_instance_type='ml.c4.xlarge', output_path=output_loc_cases, \n",
    "                                         sagemaker_session=sess)\n",
    "estimator_cases.set_hyperparameters(feature_dim=train_features.shape[1],\n",
    "                        predictor_type='regressor',\n",
    "                        num_factors=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_deaths = sage.estimator.Estimator(container, role, train_instance_count=1,\n",
    "                                         train_instance_type='ml.c4.xlarge', output_path=output_loc_deaths, \n",
    "                                         sagemaker_session=sess)\n",
    "estimator_deaths.set_hyperparameters(feature_dim=train_features.shape[1],\n",
    "                        predictor_type='regressor',\n",
    "                        num_factors=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training job for cases\n",
    "# don't run if not needed\n",
    "\n",
    "estimator_cases.fit({'train': s3_train_data_cases, 'validation': s3_test_data_cases}, wait=False)\n",
    "training_job_name_cases = estimator_cases.latest_training_job.job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training job for deaths\n",
    "# don't run if not needed\n",
    "\n",
    "estimator_deaths.fit({'train': s3_train_data_deaths, 'validation': s3_test_data_deaths}, wait=False)\n",
    "training_job_name_deaths = estimator_deaths.latest_training_job.job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_job_status(training_job_name: str):\n",
    "    job_info = boto3.client('sagemaker').describe_training_job(TrainingJobName=training_job_name)\n",
    "    job_status = job_info['TrainingJobStatus']\n",
    "    if job_status == 'Failed':\n",
    "        message = job_info['FailureReason']\n",
    "        print(f'Training failed with the following error: {message}')\n",
    "    return job_status, job_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Completed',\n",
       " {'TrainingJobName': 'factorization-machines-2020-04-05-12-49-51-846',\n",
       "  'TrainingJobArn': 'arn:aws:sagemaker:us-east-2:022575370123:training-job/factorization-machines-2020-04-05-12-49-51-846',\n",
       "  'ModelArtifacts': {'S3ModelArtifacts': 's3://sagemaker-studio-uok86wzhfvl/covid-data/output_deaths/factorization-machines-2020-04-05-12-49-51-846/output/model.tar.gz'},\n",
       "  'TrainingJobStatus': 'Completed',\n",
       "  'SecondaryStatus': 'Completed',\n",
       "  'HyperParameters': {'feature_dim': '502',\n",
       "   'num_factors': '20',\n",
       "   'predictor_type': 'regressor'},\n",
       "  'AlgorithmSpecification': {'TrainingImage': '404615174143.dkr.ecr.us-east-2.amazonaws.com/factorization-machines:1',\n",
       "   'TrainingInputMode': 'File',\n",
       "   'MetricDefinitions': [{'Name': 'train:rmse:epoch',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, train rmse <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:progress',\n",
       "     'Regex': '#progress_metric: host=\\\\S+, completed (\\\\S+) %'},\n",
       "    {'Name': 'test:binary_f_beta',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test binary_f_1.000 <score>=(\\\\S+)'},\n",
       "    {'Name': 'test:mse',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test mse <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:binary_classification_accuracy:batch',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, batch=\\\\S+ train binary_classification_accuracy <score>=(\\\\S+)'},\n",
       "    {'Name': 'train:mse:batch',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, batch=\\\\S+ train mse <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:absolute_loss:epoch',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, train absolute_loss <loss>=(\\\\S+)'},\n",
       "    {'Name': 'test:absolute_loss',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test absolute_loss <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:mse',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, train mse <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:binary_f_beta:batch',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, batch=\\\\S+ train binary_f_1.000 <score>=(\\\\S+)'},\n",
       "    {'Name': 'train:binary_classification_cross_entropy:epoch',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, train binary_classification_cross_entropy <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:binary_f_beta',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, train binary_f_1.000 <score>=(\\\\S+)'},\n",
       "    {'Name': 'train:mse:epoch',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, train mse <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:rmse:batch',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, batch=\\\\S+ train rmse <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:binary_classification_cross_entropy',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, train binary_classification_cross_entropy <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:binary_f_beta:epoch',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, train binary_f_1.000 <score>=(\\\\S+)'},\n",
       "    {'Name': 'train:rmse',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, train rmse <loss>=(\\\\S+)'},\n",
       "    {'Name': 'test:rmse',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test rmse <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:binary_classification_accuracy:epoch',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, train binary_classification_accuracy <score>=(\\\\S+)'},\n",
       "    {'Name': 'train:absolute_loss:batch',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, batch=\\\\S+ train absolute_loss <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:binary_classification_accuracy',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, train binary_classification_accuracy <score>=(\\\\S+)'},\n",
       "    {'Name': 'test:binary_classification_cross_entropy',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test binary_classification_cross_entropy <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:absolute_loss',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, train absolute_loss <loss>=(\\\\S+)'},\n",
       "    {'Name': 'train:throughput',\n",
       "     'Regex': '#throughput_metric: host=\\\\S+, train throughput=(\\\\S+) records/second'},\n",
       "    {'Name': 'test:binary_classification_accuracy',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, test binary_classification_accuracy <score>=(\\\\S+)'},\n",
       "    {'Name': 'train:binary_classification_cross_entropy:batch',\n",
       "     'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, batch=\\\\S+ train binary_classification_cross_entropy <loss>=(\\\\S+)'}],\n",
       "   'EnableSageMakerMetricsTimeSeries': False},\n",
       "  'RoleArn': 'arn:aws:iam::022575370123:role/service-role/AmazonSageMaker-ExecutionRole-20200402T213912',\n",
       "  'InputDataConfig': [{'ChannelName': 'train',\n",
       "    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "      'S3Uri': 's3://sagemaker-studio-uok86wzhfvl/covid-data/train_deaths/fm',\n",
       "      'S3DataDistributionType': 'FullyReplicated'}},\n",
       "    'CompressionType': 'None',\n",
       "    'RecordWrapperType': 'None'},\n",
       "   {'ChannelName': 'validation',\n",
       "    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "      'S3Uri': 's3://sagemaker-studio-uok86wzhfvl/covid-data/test_deaths/fm',\n",
       "      'S3DataDistributionType': 'FullyReplicated'}},\n",
       "    'CompressionType': 'None',\n",
       "    'RecordWrapperType': 'None'}],\n",
       "  'OutputDataConfig': {'KmsKeyId': '',\n",
       "   'S3OutputPath': 's3://sagemaker-studio-uok86wzhfvl/covid-data/output_deaths'},\n",
       "  'ResourceConfig': {'InstanceType': 'ml.c4.xlarge',\n",
       "   'InstanceCount': 1,\n",
       "   'VolumeSizeInGB': 30},\n",
       "  'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "  'CreationTime': datetime.datetime(2020, 4, 5, 12, 49, 52, tzinfo=tzlocal()),\n",
       "  'TrainingStartTime': datetime.datetime(2020, 4, 5, 12, 51, 27, 276000, tzinfo=tzlocal()),\n",
       "  'TrainingEndTime': datetime.datetime(2020, 4, 5, 12, 52, 34, 493000, tzinfo=tzlocal()),\n",
       "  'LastModifiedTime': datetime.datetime(2020, 4, 5, 12, 52, 34, 493000, tzinfo=tzlocal()),\n",
       "  'SecondaryStatusTransitions': [{'Status': 'Starting',\n",
       "    'StartTime': datetime.datetime(2020, 4, 5, 12, 49, 52, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 5, 12, 51, 27, 276000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Preparing the instances for training'},\n",
       "   {'Status': 'Downloading',\n",
       "    'StartTime': datetime.datetime(2020, 4, 5, 12, 51, 27, 276000, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 5, 12, 52, 8, 913000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Downloading input data'},\n",
       "   {'Status': 'Training',\n",
       "    'StartTime': datetime.datetime(2020, 4, 5, 12, 52, 8, 913000, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 5, 12, 52, 27, 680000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Training image download completed. Training in progress.'},\n",
       "   {'Status': 'Uploading',\n",
       "    'StartTime': datetime.datetime(2020, 4, 5, 12, 52, 27, 680000, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 5, 12, 52, 34, 493000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Uploading generated training model'},\n",
       "   {'Status': 'Completed',\n",
       "    'StartTime': datetime.datetime(2020, 4, 5, 12, 52, 34, 493000, tzinfo=tzlocal()),\n",
       "    'EndTime': datetime.datetime(2020, 4, 5, 12, 52, 34, 493000, tzinfo=tzlocal()),\n",
       "    'StatusMessage': 'Training job completed'}],\n",
       "  'FinalMetricDataList': [{'MetricName': 'train:rmse:epoch',\n",
       "    'Value': 2417065459712.0,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 34, 51, 145000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:progress',\n",
       "    'Value': 100.0,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 34, 51, 145000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:mse:batch',\n",
       "    'Value': 6.012083853380351e+24,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 34, 51, 145000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:absolute_loss:epoch',\n",
       "    'Value': 754118688768.0,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 34, 51, 145000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:mse',\n",
       "    'Value': 5.842205481362551e+24,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 34, 51, 145000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:mse:epoch',\n",
       "    'Value': 5.842205481362551e+24,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 34, 51, 145000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:rmse:batch',\n",
       "    'Value': 2451954991104.0,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 34, 51, 145000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:rmse',\n",
       "    'Value': 2417065459712.0,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 34, 51, 145000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:absolute_loss:batch',\n",
       "    'Value': 775546339328.0,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 34, 51, 145000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:absolute_loss',\n",
       "    'Value': 754118688768.0,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 34, 51, 145000, tzinfo=tzlocal())},\n",
       "   {'MetricName': 'train:throughput',\n",
       "    'Value': 31422.462890625,\n",
       "    'Timestamp': datetime.datetime(1970, 1, 19, 8, 34, 51, 145000, tzinfo=tzlocal())}],\n",
       "  'EnableNetworkIsolation': False,\n",
       "  'EnableInterContainerTrafficEncryption': False,\n",
       "  'EnableManagedSpotTraining': False,\n",
       "  'TrainingTimeInSeconds': 67,\n",
       "  'BillableTimeInSeconds': 67,\n",
       "  'ResponseMetadata': {'RequestId': 'ead3e41c-f691-47e7-b387-c5ebe1e1ff57',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amzn-requestid': 'ead3e41c-f691-47e7-b387-c5ebe1e1ff57',\n",
       "    'content-type': 'application/x-amz-json-1.1',\n",
       "    'content-length': '6824',\n",
       "    'date': 'Sun, 05 Apr 2020 13:00:14 GMT'},\n",
       "   'RetryAttempts': 0}})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_training_job_status(estimator_deaths.latest_training_job.job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor_cases = estimator_cases.deploy(initial_instance_count=1, wait=False, \n",
    "                                   instance_type='ml.c4.xlarge', endpoint_name='fm-covid19-cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor_cases.content_type = 'application/json'\n",
    "fm_predictor_cases.serializer = json_serializer\n",
    "fm_predictor_cases.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor_deaths = estimator_deaths.deploy(initial_instance_count=1, wait=False,\n",
    "                                   instance_type='ml.c4.xlarge', endpoint_name='fm-covid19-deaths')\n",
    "fm_predictor_deaths.content_type = 'application/json'\n",
    "fm_predictor_deaths.serializer = csv_serializer\n",
    "fm_predictor_deaths.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join socioeconomic measures by county together\n",
    "\n",
    "data_with_edu_unemploy = pd.merge(education, unemployment, how='left', left_on='FIPS Code', right_on='FIPS')\n",
    "data_with_edu_unemploy_pov = pd.merge(\n",
    "    data_with_edu_unemploy, poverty, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_pop = pd.merge(\n",
    "    data_with_edu_unemploy_pov, population, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_people = pd.merge(\n",
    "    data_after_pop, atlas_people, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_jobs = pd.merge(\n",
    "    data_after_atlas_people, atlas_jobs, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_classif = pd.merge(\n",
    "    data_after_atlas_jobs, atlas_county_classifications, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_income = pd.merge(\n",
    "    data_after_atlas_classif, atlas_income, how='left', left_on='FIPS', right_on='FIPS')\n",
    "data_after_atlas_veterans = pd.merge(\n",
    "    data_after_atlas_income, atlas_veterans, how='left', left_on='FIPS', right_on='FIPS')\n",
    "county_data = data_after_atlas_veterans.drop(columns=['FIPS Code'])\n",
    "county_data = county_data.set_index('FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker-runtime')\n",
    "def linear_predictor(feature, label='cases'):\n",
    "    csv_feature = StringIO('')\n",
    "    df_temp = pd.DataFrame(feature)\n",
    "    df_temp.transpose().to_csv(csv_feature, header=False, index=False)\n",
    "    response = client.invoke_endpoint(EndpointName='predict-covid19-{}'.format(label),\n",
    "                          Body=csv_feature.getvalue(), ContentType='text/csv')\n",
    "    csv_feature.close()\n",
    "    return json.load(response['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_predictor(feature, label='cases'):\n",
    "    csv_feature = StringIO('')\n",
    "    df_temp = pd.DataFrame(feature)\n",
    "    df_temp.transpose().to_csv(csv_feature, header=False, index=False)\n",
    "    response = client.invoke_endpoint(EndpointName='xgboost-covid19-{}'.format(label),\n",
    "                                     Body=csv_feature.getvalue(), ContentType='text/csv')\n",
    "    csv_feature.close()\n",
    "    return json.load(response['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_predictor(feature, label='cases'):\n",
    "    data = {'instances': [{'features': np.asarray(feature).astype(float).tolist()}]}\n",
    "    response = client.invoke_endpoint(EndpointName='fm-covid19-{}'.format(label),\n",
    "                          Body=json.dumps(data), ContentType='application/json')\n",
    "    return json.load(response['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a feature should have: date (Unix format), fips number, county data\n",
    "def get_linear_prediction(date, fips, label='cases'):\n",
    "    a = np.array([date, fips], dtype=np.float32)\n",
    "    b = county_data.loc[fips].to_numpy(np.float32)\n",
    "    feature = np.concatenate((a, b))\n",
    "    return linear_predictor(feature, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a feature should have: date (Unix format), fips number, county data\n",
    "def get_xgboost_prediction(date, fips, label='cases'):\n",
    "    a = np.array([date, fips], dtype=np.float32)\n",
    "    b = county_data.loc[fips].to_numpy(np.float32)\n",
    "    feature = np.concatenate((a, b))\n",
    "    return xgboost_predictor(feature, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a feature should have: date (Unix format), fips number, county data\n",
    "def get_fm_prediction(date, fips, label='cases'):\n",
    "    a = np.array([date, fips], dtype=np.float32)\n",
    "    b = county_data.loc[fips].to_numpy(np.float32)\n",
    "    feature = np.concatenate((a, b))\n",
    "    return fm_predictor(feature, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'score': 166.59375}]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_linear_prediction((datetime.now() + timedelta(0)).timestamp(), 39103, label='cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.50457000732422"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_xgboost_prediction((datetime.now() + timedelta(-10)).timestamp(), 39103, label='cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'score': -201273868288.0}]}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fm predictions are ridiculously off\n",
    "\n",
    "get_fm_prediction((datetime.now() + timedelta(-10)).timestamp(), 39103, label='deaths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:environment/datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
